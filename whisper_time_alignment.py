"""
Align transcript generated by Whisper model with the corresponding audio at the word level using Dynamic Time Warping.
Based on this project: https://github.com/linto-ai/whisper-timestamped

This is not forced-alignment, but just timestamping the output of the Whisper acoustic model using Dynamic Time Warping.
"""

import os
import sys
import time
import json
import torch
import logging
import argparse
from tqdm import tqdm
from Tools import utils
from pathlib import Path
import whisper_timestamped as whisper


def normalize_timestamp_output(segments):
    """
    Args:
        segments (List[Dict]):
            List of segments in the transcript, where each segment is a Dict that encompasses a few words in the transcript, or the entire transcript if there is only 1 segment.

    Returns:
        values (List[Dict]):
            List of dict objects where each dict has the following fields:
                'word': [str] the word itself.
                'start_time': [float] the start time in seconds of the word in the corresponding audio file.
                'end_time': [float] the end time in seconds of the word in the corresponding audio file.
    """
    values = []
    for segment in segments:
        for word in segment['words']:
            word_dict = dict()
            word_dict['word'] = word['text']
            word_dict['start_time'] = word['start']
            word_dict['end_time'] = word['end']
            values.append(word_dict)
    
    return values


def run_inference(model, beam_size, speech_files):
    """Runs whisper inference sequentially on all audio files.

    Args:
      model [Whisper].
      beam_size [int]:
        used for beam search decoding.
      speech_files [List[str]]:
        A sorted list of speech file paths.

    Returns:
      transcripts [List[List[Dict]]]:
        A List of Lists, where each List corresponds to one audio file. Each List contains a Dict for each beam hypothesis for that audio file (number of Dicts = --beam_size).
        Dict objects have the following fields of interest:
            'text': [str] the transcript of the audio, produced by that beam.
            'segments'[0] -> 'words': [List[Dict]] a List of Dicts for each word in the transcript, with the fields being:
                'text': [str] the word itself.
                'start': [float] the start time of the word in the audio file in seconds.
                'end': [float] the end time of the word in the audio file in seconds.
    """
    with torch.inference_mode():
        # result object per audio file.
        transcripts = []
        # loop through audio files
        for speech_filename in tqdm(speech_files, total=len(speech_files), unit=" audio files", desc=f"processing audio files, so far"):
            logging.info(f"processing audio file {speech_filename}")

            audio = whisper.load_audio(speech_filename)
            # return all hypotheses from beam search ranked from most likely as index 0 to least likely as index -1.
            results = whisper.transcribe(model, audio, beam_size=beam_size, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))

            transcripts.append(results)

            logging.info(f"finished processing audio file {speech_filename}")

        return transcripts


def main(args):
    "Setup and use whisper model for time alignment between predicted transcript and audio file."

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = whisper.load_model(args.model_path, device=device)

    # get all wav files as strings.
    wav_paths = list(map(lambda x: str(x), list(Path(args.in_dir).glob("**/*.wav"))))
    # filter out any undesired wav files.
    if args.path_filters:
        for fil in args.path_filters:
            wav_paths = [w for w in wav_paths if fil not in w]
    
    wav_paths.sort()

    # run ASR inference and decode into predicted hypothesis transcripts.
    hypos = run_inference(model, args.beam_size, wav_paths)

    # populate best_hypotheses.json file (if num_hyps=1) or hypothesesX_of_N.json files (if num_hyps > 1).
    if args.num_hyps > 1:
        # save all hypotheses to files.
        try:
            for all_hyps, wav_path in zip(hypos, wav_paths):
                for i in range(args.num_hyps):
                    with open(os.path.join(args.out_dir, f"hypotheses{i+1}_of_{len(all_hyps)}.json"), 'a') as hyp_file:
                        # create unique id of audio sample by including leaf folder in the id.
                        temp = wav_path.split('/')[-2:] # [0] = subfolder, [1] = ____.wav
                        temp[-1] = temp[-1].split('.wav')[0] # remove '.wav'
                        id = '/'.join(temp)
                        item = dict()
                        item['wav_path'] = wav_path
                        item['id'] = id
                        item['pred_txt'] = all_hyps[i]['text']
                        if args.time_aligns:
                            values = normalize_timestamp_output(all_hyps[i]['segments'])
                            item['timestamps_word'] =  values
                        hyp_file.write(json.dumps(item) + "\n")
        except KeyError:
            logging.error("Cannot return multiple hypotheses because the original 'whisper' and 'whisper-timestamped' libraries do not provide this functionality. Need to manually edit libraries' implementations -> Please use forked code from https://github.com/abarcovschi/whisper-fork and https://github.com/abarcovschi/whisper-timestamped-fork to make this script work with '--num_hyps' > 1.")
            sys.exit(1)
    else:
        with open(os.path.join(args.out_dir, "best_hypotheses.json"), 'w') as hyp_file:
            for all_hyps, wav_path in zip(hypos, wav_paths):
                # create unique id of audio sample by including leaf folder in the id.
                temp = wav_path.split('/')[-2:] # [0] = subfolder, [1] = ____.wav
                temp[-1] = temp[-1].split('.wav')[0] # remove '.wav'
                id = '/'.join(temp)
                item = dict()
                item['wav_path'] = wav_path
                item['id'] = id
                # use just the best hypothesis.
                item['pred_txt'] = all_hyps[0]['text']
                if args.time_aligns:
                    values = normalize_timestamp_output(all_hyps[i]['segments'])
                    item['timestamps_word'] =  values
                hyp_file.write(json.dumps(item) + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run ASR inference (decoding) using Whisper ASR model and perform time alignment on folder(s) in the dataset.")
    parser.add_argument("--in_dir", type=str, required=True,
                        help="Path to an existing folder containing wav audio files, optionally with corresponding txt transcript files for the corresponding audio files.")
    parser.add_argument("--out_dir", type=str, required=True,
                        help="Path to a new output folder to create, where results will be saved.")
    parser.add_argument("--model_path", type=str, default='',
                        help="Path of a huggingface cloud-hosted Whisper model.")
    parser.add_argument("--beam_size", type=int, default=5,
                        help="Length of beam for beam search decoding. Defaults to 5 (consistent with Whisper paper).")
    parser.add_argument("--num_hyps", type=int, default=1,
                    help="The number of best hypotheses to be returned by beam search decoding (if using beam search decoder) for an audio file. Defaults to 1 (i.e. returns just the best hypothesis). NOTE: this does not change beam size of decoder!")
    parser.add_argument("--time_aligns", default=False, action='store_true',
                        help="Flag used to specify whether to save word-level time alignment information along with the transcript for the hypothesis/hypotheses. Defaults to False if flag is not provided.")
    parser.add_argument("--path_filters", type=str, nargs='+', default='',
                        help="List of keywords to filter the paths to audio files in the 'folder' directory. Will filter out any auidio files that have those keywords present anywhere in their absolute path.")

    # parse command line arguments
    args = parser.parse_args()

    # check arg vals if in allowable range.
    if args.beam_size < 1:
        raise ValueError("'--beam_size' should be a value >= 1 !!!")
    if args.num_hyps < 1:
        raise ValueError("'--num_hyps' should be a value >= 1 !!!")
    if args.num_hyps > args.beam_size:
        raise ValueError("'--num_hyps' should be a value <= '--beam_size' !!!")

    # setup logging to both console and logfile
    utils.setup_logging(args.out_dir, 'whisper_time_alignment.log', console=True)

    p = utils.Profiler()
    p.start()

    main(args)

    p.stop()