# align transcript generated by Whisper model with the corresponding audio at the word level using Dynamic Time Warping.
# based on this project: https://github.com/linto-ai/whisper-timestamped

# this is not forced-alignment, but just timestamping the output of the whisper acoustic model.

import os
import sys
import time
import torch
import logging
import argparse
from Tools import utils
import whisper_timestamped as whisper


def run_inference_batch(root_cur_out_dir, speech_files):
    """Runs whisper inference on all audio files read from a leaf folder.

    Args:
      root_cur_out_dir (str):
        The name of the output directory into which the alignments will be saved.
      speech_files (str, list):
        A sorted list of speech file paths found in this directory.
    """
    with torch.inference_mode():
        # loop through audio files
        for speech_filename in speech_files:            
            # get speech segment index from the filename
            if ".wav" in speech_filename:
                speech_idx = speech_filename.split('.wav')[0].split('/')[-1]
            elif ".flac" in speech_filename:
                speech_idx = speech_filename.split('.flac')[0].split('/')[-1]
            else:
                speech_idx = speech_filename.split('.mp3')[0].split('/')[-1]
            # create output subfolder for an audio file
            cur_out_dir = os.path.join(root_cur_out_dir, speech_idx)
            if not os.path.exists(cur_out_dir): os.makedirs(cur_out_dir, exist_ok=True)

            audio = whisper.load_audio(speech_filename)
            result = whisper.transcribe(model, audio, beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))
            
            with open(os.path.join(cur_out_dir, 'alignments.txt'), 'w') as f:
                f.write("confidence_score,word_label,start_time,stop_time\n") # time is in seconds
                i = 0
                # for each word detected, save to file the {confidence score, label, start time, stop time} as a CSV line
                for word in result['segments'][0]['words']:
                    f.write(f"{word['confidence']:.2f},{word['text']},{word['start']:.2f},{word['end']:.2f}\n")


def run_inference():
    """Runs whisper model on a folder specified as the positional argument of this script.

    The script can be run on either one leaf folder (specify the 'leaf' --mode at the command line) or on a root folder, where there are multiple leaf folders,
     effectively running the script over the entire dataset (specified the 'root' --mode at the command line).
    """
    # if mode=leaf run the script only on the audio files in a single folder specified
    # if mode=root, run the script on all subfolders, essentially over the entire dataset

    for dirpath, _, filenames in os.walk(args.folder, topdown=asleaf): # if topdown=True, read contents of folder before subfolders, otherwise the reverse logic applies
        # if this script was run previously, an output folder will be present in the folder where the audio files we want to process are, skip it and its subfolders
        if WHISPER_ALIGNS_PREFIX not in dirpath:
            # process only those folders that contain a audio files
            audio_files = list(filter(lambda filepath: filepath.endswith('.flac') or filepath.endswith('.wav') or filepath.endswith('.mp3'), filenames))
            audio_files = list(map(lambda filepath: os.path.join(dirpath, filepath), audio_files))
            if audio_files:
                logging.info(f"starting to process folder {dirpath}")
                # create root output dir (specified by global out_dir)
                cur_out_dir = os.path.join(dirpath, out_dir)
                if not os.path.exists(cur_out_dir): os.makedirs(cur_out_dir, exist_ok=True)
                # run whisper
                run_inference_batch(cur_out_dir, audio_files)
                logging.info(f"finished processing folder {dirpath}")
            if asleaf:
                break # to prevent reading subfolders


def main():
    "Setup and use whisper model for time alignment between predicted transcript and audio file."
    global model

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = whisper.load_model(args.model_path, device=device)

    run_inference()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run ASR inference (decoding) using Whisper ASR model and perform time alignment on folder(s) in the dataset.")
    parser.add_argument("folder", type=str, nargs='?', default=os.getcwd(),
                        help="Path to a folder containing audio files to transcribe and align. Defaults to CWD if not provided.")
    parser.add_argument("--model_path", type=str, default='',
                        help="Path of a huggingface cloud-hosted Whisper model.")
    parser.add_argument("--out_folder_name", type=str, default='whisper_alignments',
                    help="Name of the output folder, useful to differentiate runs.")
    parser.add_argument("--mode", type=str, choices={'leaf', 'root'}, default="root",
                        help="Specifies how the folder will be processed.\nIf 'leaf': only the folder will be searched for audio files (single folder inference),\nIf 'root': subdirs are searched (full dataset inference).\nDefaults to 'root' if unspecified.")

    # parse command line arguments
    global args
    args = parser.parse_args()
    
    # setup folder structure variables
    global out_dir, WHISPER_ALIGNS_PREFIX
    WHISPER_ALIGNS_PREFIX = "WHISPER_ALIGNS_"
    out_dir = WHISPER_ALIGNS_PREFIX + args.out_folder_name # the output folder to be created in folders where there are audio files and a transcript file

    # setup logging to both console and logfile
    utils.setup_logging(args.folder, 'whisper_time_alignment.log', console=True)

    # setup directory traversal mode variables
    mode = args.mode
    global asleaf
    asleaf = True if mode == 'leaf' else False

    # start timing how long it takes to run script
    tic = time.perf_counter()

    # log the command that started the script
    logging.info(f"Started script via: python {' '.join(sys.argv)}")
    main()

    toc = time.perf_counter()
    logging.info(f"Finished processing in {time.strftime('%H:%M:%Ss', time.gmtime(toc - tic))}")