# align transcript generated by Whisper model with the corresponding audio at the word level using Dynamic Time Warping.
# based on this project: https://github.com/linto-ai/whisper-timestamped

# this is not forced-alignment, but just timestamping the output of the whisper acoustic model.

import os
import sys
import time
import torch
import string
import logging
import argparse
from tqdm import tqdm
from Tools import utils
import whisper_timestamped as whisper


def run_inference_batch(speech_files):
    """Runs whisper inference on all audio files read from a leaf folder.

    Args:
      speech_files (str, list):
        A sorted list of speech file paths found in this directory.
    """
    with torch.inference_mode():
        # loop through audio files
        for speech_filename in tqdm(speech_files, total=len(speech_files), unit=" audio files", desc=f"processing audio files in folder, so far"):
            logging.info(f"processing audio file {speech_filename}")
            # create output subfolder for an audio file.
            '/'.join(speech_filename.split('/')[:-1])
            cur_out_dir = os.path.join('/'.join(speech_filename.split('/')[:-1]), out_dir)
            if not os.path.exists(cur_out_dir): os.makedirs(cur_out_dir, exist_ok=True)
            cur_out_dir = os.path.join(cur_out_dir, speech_filename.split('/')[-1].split(".flac")[0]) if ".flac" in speech_filename else os.path.join(cur_out_dir, speech_filename.split('/')[-1].split(".wav")[0])
            if not os.path.exists(cur_out_dir): os.makedirs(cur_out_dir, exist_ok=True)

            audio = whisper.load_audio(speech_filename)
            # return all hypotheses from beam search ranked from most likely as index 0 to least likely as index -1.
            results = whisper.transcribe(model, audio, beam_size=args.beam_size, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))
            
            if not args.all_hypotheses:
            # only save the best hypothesis to file.
                with open(os.path.join(cur_out_dir, f"alignments_beamsearch_best.txt"), 'w') as f:
                    segments = results[0]['segments'] if type(results) == list else results['segments']
                    f.write("confidence_score,word_label,start_time,stop_time\n") # time is in seconds
                    for segment in segments:
                        # for each word, save to file the {confidence score, word label, start time, stop time} as a CSV line
                        for word in segment['words']:
                            # remove punctuation and convert to lower case
                            stripped_text = word['text'].translate(str.maketrans('', '', string.punctuation)).lower()
                            f.write(f"{word['confidence']:.2f},{stripped_text},{word['start']:.2f},{word['end']:.2f}\n")
            # save all hypotheses to files.
            else:
                try:
                    for i in range(len(results)):
                        with open(os.path.join(cur_out_dir, f"alignments_beamsearch_{i+1}_of_{len(results)}.txt"), 'w') as f:
                            f.write("confidence_score,word_label,start_time,stop_time\n") # time is in seconds
                            for segment in results[i]['segments']:
                                # for each word, save to file the {confidence score, word label, start time, stop time} as a CSV line
                                for word in segment['words']:
                                    # remove punctuation and convert to lower case
                                    stripped_text = word['text'].translate(str.maketrans('', '', string.punctuation)).lower()
                                    f.write(f"{word['confidence']:.2f},{stripped_text},{word['start']:.2f},{word['end']:.2f}\n")
                except KeyError:
                    logging.error("Cannot return multiple hypotheses because the original 'whisper' and 'whisper-timestamped' libraries do not provide this functionality. Need to manually edit libraries' implementations -> Please use forked code from https://github.com/abarcovschi/whisper-fork and https://github.com/abarcovschi/whisper-timestamped-fork to make this script work with the '--all_hypotheses' flag.")
                    sys.exit(1)


def run_inference():
    """Runs whisper model on a folder specified as the positional argument of this script.

    The script can be run on either one leaf folder (specify the 'leaf' --mode at the command line) or on a root folder, where there are multiple leaf folders,
     effectively running the script over the entire dataset (specified the 'root' --mode at the command line).
    """
    # if mode=leaf run the script only on the audio files in a single folder specified
    # if mode=root, run the script on all subfolders, essentially over the entire dataset

    for dirpath, _, filenames in os.walk(args.folder, topdown=asleaf): # if topdown=True, read contents of folder before subfolders, otherwise the reverse logic applies
        # process only those folders that contain a audio files.
        audio_files = list(filter(lambda filepath: filepath.endswith('.flac') or filepath.endswith('.wav') or filepath.endswith('.mp3'), filenames))
        # get absolute paths for files.
        audio_files = list(map(lambda filepath: os.path.join(dirpath, filepath), audio_files))
        # filter out any undesired files.
        if args.path_filters:
            for path_filter in args.path_filters:
                audio_files = [filepath for filepath in audio_files if path_filter not in filepath]
        
        if audio_files:
            logging.info(f"starting to process folder {dirpath}")
            # run whisper
            run_inference_batch(audio_files)
            logging.info(f"finished processing folder {dirpath}")
        if asleaf:
            break # to prevent reading subfolders


def main():
    "Setup and use whisper model for time alignment between predicted transcript and audio file."
    global model

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = whisper.load_model(args.model_path, device=device)

    run_inference()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run ASR inference (decoding) using Whisper ASR model and perform time alignment on folder(s) in the dataset.")
    parser.add_argument("folder", type=str, nargs='?', default=os.getcwd(),
                        help="Path to a folder containing audio files to transcribe and align. Defaults to CWD if not provided.")
    parser.add_argument("--model_path", type=str, default='',
                        help="Path of a huggingface cloud-hosted Whisper model.")
    parser.add_argument("--beam_size", type=int, default=1,
                        help="Length of beam for beam search decoding. Defaults to 5 (consistent with Whisper paper).")
    parser.add_argument("--out_folder_name", type=str, default='whisper_alignments',
                        help="Name of the output folder, useful to differentiate runs. Defaults to 'whisper_alignments'.")
    parser.add_argument("--mode", type=str, choices={'leaf', 'root'}, default="root",
                        help="Specifies how the folder will be processed.\nIf 'leaf': only the folder will be searched for audio files (single folder inference),\nIf 'root': subdirs are searched (full dataset inference).\nDefaults to 'root' if unspecified.")
    parser.add_argument("--all_hypotheses", default=False, action='store_true',
                        help="Flag used to specify whether to save all hypotheses returned by beam search decoding. Defaults to False if flag is not provided (i.e. returns just the top hypothesis).")
    parser.add_argument("--path_filters", type=str, nargs='+', default='',
                        help="List of keywords to filter the paths to audio files in the 'folder' directory. Will filter out any auidio files that have those keywords present anywhere in their absolute path.")

    # parse command line arguments
    global args
    args = parser.parse_args()
    
    # setup folder structure variables
    WHISPER_ALIGNS_PREFIX = "WHISPER_ALIGNS_"
    global out_dir
    out_dir = WHISPER_ALIGNS_PREFIX + args.out_folder_name # the output folder to be created in folders where there are audio files and a transcript file

    # setup logging to both console and logfile
    utils.setup_logging(args.folder, 'whisper_time_alignment.log', console=True)

    # setup directory traversal mode variables
    mode = args.mode
    global asleaf
    asleaf = True if mode == 'leaf' else False

    # start timing how long it takes to run script
    tic = time.perf_counter()

    # log the command that started the script
    logging.info(f"Started script via: python {' '.join(sys.argv)}")
    main()

    toc = time.perf_counter()
    logging.info(f"Finished processing in {time.strftime('%H:%M:%Ss', time.gmtime(toc - tic))}")