# -*- coding: utf-8 -*-
"""
Selecting adult speaker candidates for Adult-to-Child voice conversion. Using the demo01 file from Resemblyzer github repository 
for the computation of cosine similarities between speaker embeddings:
https://github.com/resemble-ai/Resemblyzer/blob/master/demo01_similarity.py
"""

from resemblyzer import preprocess_wav, VoiceEncoder
from distutils.dir_util import copy_tree
from itertools import groupby
from pathlib import Path
from tqdm import tqdm
import numpy as np
import warnings
import operator
import os

warnings.simplefilter(action='ignore', category=FutureWarning)

# DEMO 01: shows how to compare speech segments (=utterances) between them to get a metric  
# on how similar their voices sound. We expect utterances from the same speaker to have a high 
# similarity, and those from distinct speakers to have a lower one. 

# the speaker encoder network from the d-vectors paper
encoder = VoiceEncoder()

# set the cosine similarity threshold for adult speakers
similarity_threshold = 0.75

# define the root output folder that will contain subfolders of adult speakers in Librispeech format
# the adult speakers selected will be the ones with the highest similarity to child speakers.
# we will build this folder using this script.
OUTPUTDIR = f"/workspace/projects/Alignment/wav2vec2_alignment/speaker_encoder_outputs/Librispeech-similarity-above-{similarity_threshold}"
if not os.path.exists(OUTPUTDIR): os.makedirs(OUTPUTDIR, exist_ok=True)

# Librispeech format
# ADULTS_CORPUS = "/workspace/datasets/LibriTTS-train-clean-360/LibriTTS/train-clean-360" # LibriTTS path
ADULTS_CORPUS = "/workspace/datasets/LibriSpeech-train-clean-360/LibriSpeech/train-clean-360" # Librispeech path
# CMU Kids format
KIDS_CORPUS = "/workspace/datasets/cmu_kids_test"
# specify if grouping all child speakers into one 'CMU speaker'
avg_child = True

# get a list of all audio files in all subdirs of root KIDS_CORPUS folder
kids_wav_fpaths = list(Path(KIDS_CORPUS).glob("**/*.wav"))
assert len(kids_wav_fpaths) != 0, "Kids speech files list is empty. Check the extension used in search."
kids_wav_fpaths.sort()

# get a list of all audio files in all subdirs of root ADULTS_CORPUS folder
adults_wav_fpaths = list(Path(ADULTS_CORPUS).glob("**/*.flac"))
assert len(adults_wav_fpaths) != 0, "Adults speech files list is empty. Check the extension used in search."

#open text file containing Librispeech Speaker IDs and genders
speakers_info = open(os.path.join(Path(ADULTS_CORPUS).parent, "SPEAKERS.TXT"), 'r')


# Group the wavs per speaker and load them into arrays using the preprocessing function provided with 
# Resemblyzer to load wavs into memory. This normalizes the volume, trims long silences and resamples 
# the wav to the correct sampling rate. The grouped, preprocessed audio files will be ready to input to encoder
if avg_child:
    # Grouping kids audio samples by each speaker folder's commonly named 'signal' recording session subfolder
    #  effectively combining all child speakers into one folder -> will produce one embedding representing the average CMU child.
    kids_speaker_wavs = {speaker: list(map(preprocess_wav, wav_fpaths)) for speaker, wav_fpaths in
                    groupby(tqdm(kids_wav_fpaths, "Preprocessing kids wavs", len(kids_wav_fpaths), unit="wavs"), 
                            lambda wav_fpath: wav_fpath.parent.stem)}
else:
    # grouping kids audio samples by speaker ID -> will produce an average embedding for each child speaker
    kids_speaker_wavs = {speaker: list(map(preprocess_wav, wav_fpaths)) for speaker, wav_fpaths in
                    groupby(tqdm(kids_wav_fpaths, "Preprocessing kids wavs", len(kids_wav_fpaths), unit="wavs"), 
                            lambda wav_fpath: wav_fpath.parent.parent.stem)}

# grouping adults audio samples by speaker ID -> will produce an average embedding for each adult speaker
adults_speaker_wavs = {speaker: list(map(preprocess_wav, wav_fpaths)) for speaker, wav_fpaths in
                groupby(tqdm(adults_wav_fpaths, "Preprocessing adults wavs", len(adults_wav_fpaths), unit="wavs"), 
                        lambda wav_fpath: wav_fpath.parent.parent.stem)}

# for each speaker, create an averaged speaker embedding vector of len 256
#  by averaging all the individual embeddings generated by speaker encoder for each audio sample of that speaker.
# applies L2 norm to averages after.
# each embeddings matrix is of shape (num_speakers, embed_size).
# one matrix for the kids speakers and one for the adults.
embeds_a = np.array([encoder.embed_speaker(wavs[:len(wavs)]) \
                         for wavs in kids_speaker_wavs.values()])
embeds_b = np.array([encoder.embed_speaker(wavs[:len(wavs)]) \
                         for wavs in adults_speaker_wavs.values()])

# read info about all speakers from the entire Librispeech collection
ids = []
genders = []
for line in speakers_info.readlines():
    if ";" in line:
        pass
    else:
        l = line.split("|")
        ids.append(l[0].strip())
        genders.append(l[1].strip())

# create dict of speaker IDs and their gender
# key=id, value=gender
ids_and_genders_dict = dict(zip(ids, genders))

# dict of 'speaker ID: similarity score' pairs for adult speakers that pass the similarity score threshold
adults_high_similarity = {}

# Compute the similarity matrix. The similarity of two embeddings is simply their dot 
# product, because the similarity metric is the cosine similarity and the embeddings are 
# already L2-normed.

# Short version:
 #utt_sim_matrix = np.inner(embeds_a, embeds_b)

# Long, detailed version:
# utt_sim_matrix2[i,j] will store similarity between adult speaker i and child speaker j
utt_sim_matrix2 = np.zeros((len(embeds_b), len(embeds_a)))
# # loop through each child average embedding
# for i in range(len(embeds_a)):
#     child_speaker_id = list(kids_speaker_wavs.keys())[i]
#     print(f"---- current child speaker ID: {child_speaker_id} ----")
#     # loop through each adult average embedding
#     for j in range(len(embeds_b)):
#         adult_speaker_id = list(adults_speaker_wavs.keys())[j]
#         # The @ notation is exactly equivalent to np.dot(embeds_a[i], embeds_b[i])
#         utt_sim_matrix2[i, j] = embeds_a[i] @ embeds_b[j]
#         print(f"Similarity score with adult speaker ID {adult_speaker_id} ----> {str(utt_sim_matrix2[i,j])}")
#         #f.write(str(list(adults_speaker_wavs.keys())[j]) + "---->" + str(utt_sim_matrix2[i,j])+'\n')
#         # if similarity between child speaker and adult speaker is high
#         if (utt_sim_matrix2[i,j] > similarity_threshold):
#             print("High similarity!")
#             # get path to adult speaker folder
#             adult_path = os.path.join(ADULTS_CORPUS, adult_speaker_id)
#             # copy the source adult speaker folder (which contains recording sessions subfolders) to OUTPUTDIR
#             copy_tree(adult_path, OUTPUTDIR)

# loop through each adult average embedding
for i in range(len(embeds_b)):
    adult_speaker_id = list(adults_speaker_wavs.keys())[i]
    print(f"---- current adult speaker ID: {adult_speaker_id} ----")
    # loop through each child average embedding
    for j in range(len(embeds_a)):
        child_speaker_id = list(kids_speaker_wavs.keys())[j]
        # The @ notation is exactly equivalent to np.dot(embeds_a[i], embeds_b[i])
        utt_sim_matrix2[i,j] = embeds_a[j] @ embeds_b[i]
        print(f"Similarity score with child speaker ID '{child_speaker_id}' ----> {str(utt_sim_matrix2[i,j])}")
        # if similarity between child speaker and adult speaker is high
        if (utt_sim_matrix2[i,j] > similarity_threshold):
            print(f"High similarity! (>{similarity_threshold})")
            adults_high_similarity[adult_speaker_id] = utt_sim_matrix2[i,j]
            # get path to adult speaker folder
            adult_path = os.path.join(ADULTS_CORPUS, adult_speaker_id)
            # copy everything inside the adult speaker folder (which contains recording sessions subfolders) into OUTPUTDIR/adult_speaker_id
            copy_tree(adult_path, os.path.join(OUTPUTDIR, adult_speaker_id))

speaker_folders = next(os.walk(OUTPUTDIR))[1] # get the speaker folders names we copied

# sort similarities in descending order, using sorted() for backwards compatibility or <Py3.7 versions, returns a sorted list of tuples.
adults_high_similarity_sorted = sorted(adults_high_similarity.items(), key=operator.itemgetter(1), reverse=True)

with open(os.path.join(OUTPUTDIR, 'speaker_similarities.txt'), "w") as f:
    if avg_child:
        f.write("Using one child speaker: the average CMU child.\n")
    f.write(f"Similarity scores of adult speakers from {ADULTS_CORPUS} that are above {similarity_threshold}.\n")
    f.write(f"Total number of such speakers: {len(adults_high_similarity_sorted)} out of {len(adults_speaker_wavs)}.\n")
    f.write(f"Speaker ID, Gender, Similarity score\n")
    for id, sim in adults_high_similarity_sorted:
        f.write(f"{id},{ids_and_genders_dict[id]},{sim}\n")

#Rename the output adult speaker folders (adding gender '_m' or '_f' to the speaker id folder name)
for speaker_folder in speaker_folders:
    dir_path = os.path.join(OUTPUTDIR, speaker_folder)
    os.rename(dir_path, dir_path + "_" + ids_and_genders_dict[speaker_folder].lower())
