"""
Align transcript generated by Whisper model with the corresponding audio at the word level using Dynamic Time Warping.
Based on this project: https://github.com/linto-ai/whisper-timestamped

This is not forced-alignment, but just timestamping the output of the Whisper acoustic model using Dynamic Time Warping.
"""


import os
import sys
import torch
import logging
import argparse
from tqdm import tqdm

current_dir = os.path.dirname(os.path.realpath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from Utils import utils
from typing import List, Union, Dict
import whisper_timestamped as whisper
from Utils.asr.common_utils import Hypotheses, Hypothesis, WordAlign, write_results, get_all_wavs


def format_whisper_output(whisper_output: Union[List[List[Dict]], List[Dict]], time_aligns: bool, num_hyps: int) -> List[Hypotheses]:
    """
    Convert the output of a Whisper decoder to a common format using dataclasses from 'Tools/asr/common_utils.py'.

    Args:
        whisper_output (Union[List[List[Dict]], List[Dict]]):
            The output returned by Whisper inference on a batch of audio files from 'whisper_time_alignment.py', with word-level timestamps support.
        time_aligns (bool):
            Whether to include word-level time alignments in the results.
        num_hyps (int):
            The number of top hypotheses to keep in the results.
    Returns:
        results (List[Hypotheses]):
            The common format for results for any ASR inference output.
    """
    # loop through the result for each audio file.
    results = list()
    for audio_result in whisper_output:
        hypotheses = list()
        if isinstance(audio_result, dict):
            # beam_size = 1 or using unmodified whisper-timestamped library
            word_aligns = list()
            if time_aligns:
                for segment in audio_result['segments']:
                    for word in segment['words']:
                        word_aligns.append(WordAlign(word['text'], word['start'], word['end']))
            hypotheses.append(Hypothesis(audio_result['text'].strip(), word_aligns))
        else:
            # beam_size > 1
            try:
                for i in range(num_hyps):
                    # create only as many Hypothesis objects as num_hyps.
                    hypo = audio_result[i]
                    word_aligns = list()
                    if time_aligns:
                        for segment in hypo['segments']:
                            for word in segment['words']:
                                word_aligns.append(WordAlign(word['text'], word['start'], word['end']))
                    hypotheses.append(Hypothesis(hypo['text'].strip(), word_aligns))
            except KeyError:
                logging.error("Cannot return multiple hypotheses because the original 'whisper' and 'whisper-timestamped' libraries do not provide this functionality. Need to manually edit libraries' implementations -> Please use forked code from https://github.com/abarcovschi/whisper-fork and https://github.com/abarcovschi/whisper-timestamped-fork to make this script work with '--num_hyps' > 1.")
                sys.exit(1)
        results.append(Hypotheses(hypotheses))
        
    return results


def run_inference(model, beam_size, speech_files):
    """Runs whisper inference sequentially on all audio files.

    Args:
      model [Whisper].
      beam_size [int]:
        used for beam search decoding.
      speech_files [List[str]]:
        A sorted list of speech file paths.

    Returns:
      transcripts [List[List[Dict]]]:
        A List of Lists, where each List corresponds to one audio file. Each List contains a Dict for each beam hypothesis for that audio file (number of Dicts = --beam_size).
        Dict objects have the following fields of interest:
            'text': [str] the transcript of the audio, produced by that beam.
            'segments'[0] -> 'words': [List[Dict]] a List of Dicts for each word in the transcript, with the fields being:
                'text': [str] the word itself.
                'start': [float] the start time of the word in the audio file in seconds.
                'end': [float] the end time of the word in the audio file in seconds.
    """
    with torch.inference_mode():
        # result object per audio file.
        transcripts = []
        # loop through audio files
        for speech_filename in tqdm(speech_files, total=len(speech_files), unit=" audio files", desc=f"processing audio files, so far"):
            logging.info(f"processing audio file {speech_filename}")

            audio = whisper.load_audio(speech_filename)
            # return all hypotheses from beam search ranked from most likely as index 0 to least likely as index -1.
            results = whisper.transcribe(model, audio, beam_size=beam_size, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))

            transcripts.append(results)

            logging.info(f"finished processing audio file {speech_filename}")

        return transcripts


def main(args):
    "Setup and use whisper model for time alignment between predicted transcript and audio file."

    # model setup
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = whisper.load_model(args.model_path, device=device)

    # get all wav files as strings.
    wav_paths = get_all_wavs(args.in_dir, args.path_filters)

    # run ASR inference and decode into predicted hypothesis transcripts.
    results = run_inference(model, args.beam_size, wav_paths)

    # format decoded ASR inference results to a common interface.
    results = format_whisper_output(results, args.time_aligns, args.num_hyps)

    # write results hypotheses to output json files.
    write_results(results, wav_paths, args.out_dir)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Run ASR inference (decoding) using Whisper ASR model and perform time alignment on folder(s) in the dataset.")
    parser.add_argument("--in_dir", type=str, required=True,
                        help="Path to an existing folder containing wav audio files, optionally with corresponding txt transcript files for the corresponding audio files.")
    parser.add_argument("--out_dir", type=str, required=True,
                        help="Path to a new output folder to create, where results will be saved.")
    parser.add_argument("--model_path", type=str, default='',
                        help="Path of a huggingface cloud-hosted Whisper model.")
    parser.add_argument("--beam_size", type=int, default=5,
                        help="Length of beam for beam search decoding. Defaults to 5 (consistent with Whisper paper).")
    parser.add_argument("--num_hyps", type=int, default=1,
                    help="The number of best hypotheses to be returned by beam search decoding (if using beam search decoder) for an audio file. Defaults to 1 (i.e. returns just the best hypothesis). NOTE: this does not change beam size of decoder!")
    parser.add_argument("--time_aligns", default=False, action='store_true',
                        help="Flag used to specify whether to save word-level time alignment information along with the transcript for the hypothesis/hypotheses. Defaults to False if flag is not provided.")
    parser.add_argument("--path_filters", type=str, nargs='+', default='',
                        help="List of keywords to filter the paths to audio files in the 'folder' directory. Will filter out any auidio files that have those keywords present anywhere in their absolute path.")

    # parse command line arguments
    args = parser.parse_args()

    # check arg vals if in allowable range.
    if args.beam_size < 1:
        raise ValueError("'--beam_size' should be a value >= 1 !!!")
    if args.num_hyps < 1:
        raise ValueError("'--num_hyps' should be a value >= 1 !!!")
    if args.num_hyps > args.beam_size:
        raise ValueError("'--num_hyps' should be a value <= '--beam_size' !!!")

    # setup logging to both console and logfile
    utils.setup_logging(args.out_dir, 'whisper_time_alignment.log', console=True)

    p = utils.Profiler()
    p.start()

    main(args)

    p.stop()